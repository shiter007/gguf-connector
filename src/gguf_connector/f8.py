import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import gradio as gr
import cv2
import threading
import time
import numpy as np
from PIL import Image

MODEL_ID = "callgg/fastvlm-0.5b-bf16"
IMAGE_TOKEN_INDEX = -200

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
tok = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto",
    trust_remote_code=True,
)

# å…¨å±€å˜é‡
capturing = False
latest_caption = ""
current_camera = None
cap = None
latest_frame = None

def get_available_cameras():
    """æ£€æµ‹å¯ç”¨çš„æ‘„åƒå¤´"""
    available_cameras = []
    for i in range(5):  # æ£€æµ‹å‰5ä¸ªæ‘„åƒå¤´ç´¢å¼•
        cap = cv2.VideoCapture(i)
        if cap.isOpened():
            available_cameras.append(f"Camera {i}")
            cap.release()
    return available_cameras if available_cameras else ["Camera 0"]

def initialize_camera(camera_index):
    """åˆå§‹åŒ–æ‘„åƒå¤´"""
    global cap
    if cap is not None:
        cap.release()
    
    # å°è¯•ä½¿ç”¨ä¸åŒçš„åç«¯
    backends = [cv2.CAP_AVFOUNDATION, cv2.CAP_ANY]
    
    for backend in backends:
        try:
            cap = cv2.VideoCapture(camera_index, backend)
            if cap.isOpened():
                # è®¾ç½®æ‘„åƒå¤´å‚æ•°
                cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
                cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
                cap.set(cv2.CAP_PROP_FPS, 30)
                return True
        except:
            continue
    
    # å¦‚æœéƒ½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ–¹å¼
    cap = cv2.VideoCapture(camera_index)
    if cap.isOpened():
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        return True
    
    return False

def generate_caption(img: Image.Image) -> str:
    """ä¸ºå›¾åƒç”Ÿæˆæè¿°"""
    try:
        messages = [{"role": "user", "content": "<image>\nè¯·ç”¨ä¸€å¥è¯ï¼ˆ32ä¸ªå­—ä»¥å†…ï¼‰æè¿°ä¸€ä¸‹å†…å®¹"}]
        rendered = tok.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
        pre, post = rendered.split("<image>", 1)
        
        pre_ids = tok(pre, return_tensors="pt", add_special_tokens=False).input_ids
        post_ids = tok(post, return_tensors="pt", add_special_tokens=False).input_ids
        img_tok = torch.tensor([[IMAGE_TOKEN_INDEX]], dtype=pre_ids.dtype)
        
        input_ids = torch.cat([pre_ids, img_tok, post_ids], dim=1).to(model.device)
        attention_mask = torch.ones_like(input_ids, device=model.device)
        
        px = model.get_vision_tower().image_processor(
            images=img, return_tensors="pt"
        )["pixel_values"].to(model.device, dtype=model.dtype)
        
        with torch.no_grad():
            out = model.generate(
                inputs=input_ids,
                attention_mask=attention_mask,
                images=px,
                max_new_tokens=32,
                temperature=0.7,
                do_sample=True,
            )
        
        caption = tok.decode(out[0], skip_special_tokens=True)
        return caption
    except Exception as e:
        return f"Error generating caption: {str(e)}"

def capture_loop():
    """æŒç»­æ•è·æ‘„åƒå¤´ç”»é¢å¹¶ç”Ÿæˆæè¿°"""
    global latest_caption, capturing, cap, latest_frame
    
    while capturing and cap is not None and cap.isOpened():
        try:
            ret, frame = cap.read()
            if ret:
                # ä¿å­˜æœ€æ–°å¸§ç”¨äºæ˜¾ç¤º
                latest_frame = frame.copy()
                
                # å°†OpenCVæ ¼å¼è½¬æ¢ä¸ºPILæ ¼å¼ç”¨äºç”Ÿæˆæè¿°
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                pil_image = Image.fromarray(rgb_frame)
                
                # ç”Ÿæˆæè¿°
                latest_caption = generate_caption(pil_image)
            else:
                latest_caption = "Failed to capture frame"
                
        except Exception as e:
            latest_caption = f"Error: {str(e)}"
        
        time.sleep(1)

def start_caption(camera_name):
    """å¼€å§‹å®æ—¶æè¿°"""
    global capturing, current_camera
    
    if capturing:
        return "Already running..."
    
    try:
        # ä»æ‘„åƒå¤´åç§°ä¸­æå–ç´¢å¼•
        if isinstance(camera_name, str):
            camera_index = int(camera_name.split()[-1])
        else:
            camera_index = 0
        
        if initialize_camera(camera_index):
            capturing = True
            current_camera = camera_name
            threading.Thread(target=capture_loop, daemon=True).start()
            return f"Started captioning from {camera_name}"
        else:
            return f"Failed to initialize {camera_name}"
            
    except Exception as e:
        return f"Error starting caption: {str(e)}"

def stop_caption():
    """åœæ­¢å®æ—¶æè¿°"""
    global capturing, current_camera, cap
    
    capturing = False
    if cap is not None:
        cap.release()
        cap = None
    current_camera = None
    return "Stopped captioning"

def get_caption():
    """è·å–æœ€æ–°çš„æè¿°"""
    return latest_caption if latest_caption else "Waiting for first caption..."

def get_frame():
    """è·å–æœ€æ–°çš„æ‘„åƒå¤´ç”»é¢"""
    global latest_frame
    if latest_frame is not None:
        # å°†BGRè½¬æ¢ä¸ºRGBç”¨äºæ˜¾ç¤º
        rgb_frame = cv2.cvtColor(latest_frame, cv2.COLOR_BGR2RGB)
        return rgb_frame
    else:
        # è¿”å›ä¸€ä¸ªé»‘è‰²å›¾åƒ
        return np.zeros((480, 640, 3), dtype=np.uint8)

# è·å–å¯ç”¨æ‘„åƒå¤´åˆ—è¡¨
available_cameras = get_available_cameras()

# Gradio UI
with gr.Blocks(title="Live Camera Caption by apple/fastvlm && callgg/fastvlm-0.5b-bf16") as demo:
    gr.Markdown("## ğŸ“¹ Live Camera Caption by apple/fastvlm && callgg/fastvlm-0.5b-bf16")
    gr.Markdown("Select a camera and start real-time image description")
    
    with gr.Row():
        camera_dropdown = gr.Dropdown(
            choices=available_cameras,
            value=available_cameras[0] if available_cameras else "Camera 0",
            label="Select Camera"
        )
    
    with gr.Row():
        start_btn = gr.Button("â–¶ Start Captioning", variant="primary")
        stop_btn = gr.Button("â¹ Stop Captioning", variant="stop")
    
    with gr.Row():
        with gr.Column(scale=2):
            camera_view = gr.Image(
                label="Live Camera Feed",
                type="numpy",
                height=480,
                width=640
            )
        with gr.Column(scale=1):
            output_box = gr.Textbox(
                label="Live Description",
                lines=10,
                placeholder="Camera description will appear here..."
            )
    
    # äº‹ä»¶ç»‘å®š
    start_btn.click(
        start_caption,
        inputs=[camera_dropdown],
        outputs=output_box
    )
    
    stop_btn.click(
        stop_caption,
        outputs=output_box
    )
    
    # å®šæ—¶å™¨è‡ªåŠ¨åˆ·æ–° - åŒæ—¶åˆ·æ–°æè¿°å’Œç”»é¢
    timer = gr.Timer(1)
    timer.tick(get_caption, outputs=output_box)
    timer.tick(get_frame, outputs=camera_view)
    demo.queue().launch(server_name="0.0.0.0", server_port=7860, share=False)
 